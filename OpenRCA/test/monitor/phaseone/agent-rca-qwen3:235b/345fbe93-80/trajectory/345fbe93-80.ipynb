{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {},
    "cells": [
        {
            "id": "aaf57634",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[1]:\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Possible root cause components\npossible_components = [\n    'cartservice-0', 'currencyservice-1', 'currencyservice-0', 'frontend-2', \n    'frontend-0', 'frontend-1', 'recommendationservice-2', 'adservice-0', \n    'shippingservice-0', 'cartservice-1', 'cartservice-2', 'checkoutservice-0', \n    'currencyservice-2', 'recommendationservice-1', 'shippingservice-1', \n    'shippingservice-2', 'checkoutservice-2', 'paymentservice-0', 'paymentservice-2', \n    'paymentservice-1', 'emailservice-1', 'emailservice-2', 'emailservice-0', \n    'redis-cart-0', 'adservice-1', 'adservice-2'\n]\n\n# Time conversion functions\ndef utc_str_to_timestamp(utc_str):\n    dt = datetime.strptime(utc_str, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=pytz.utc)\n    return int(dt.timestamp())\n\n# Convert UTC anomaly window to timestamps\nstart_ts = utc_str_to_timestamp('2025-06-05T16:10:02Z')\nend_ts = utc_str_to_timestamp('2025-06-05T16:31:02Z')\n\n# Base directory for metrics\nmetric_dir = 'dataset/phaseone/2025-06-06/metric-parquet/apm/pod'\n\n# Function to handle column name variations\ndef get_value_column(df):\n    \"\"\"Find a column that contains values/metrics\"\"\"\n    for col in ['value', 'values', 'metric_value', 'Value', 'Values', 'val']:\n        if col in df.columns:\n            return col\n    return None\n\n# Process each component\nbreached_components = []\n\nfor pod in possible_components:\n    # Construct file path\n    file_path = os.path.join(metric_dir, f'pod_{pod}_2025-06-06.parquet')\n    if not os.path.exists(file_path):\n        continue\n    \n    try:\n        # Read and clean data\n        df = pq.read_table(file_path).to_pandas()\n        \n        # Find appropriate columns\n        pod_col = None\n        for col in ['pod_name', 'pod', 'Pod', 'POD', 'podName']:\n            if col in df.columns:\n                pod_col = col\n                break\n        \n        value_col = get_value_column(df)\n        if value_col is None or pod_col is None:\n            continue\n            \n        df[pod_col] = df[pod_col].str.strip().str.lower()\n        target_pod = pod.strip().str.lower() if hasattr(pod, 'str') else pod.strip().lower()\n        df = df[df[pod_col] == target_pod]\n            \n        # Convert value to numeric\n        df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n        df = df.dropna()\n        \n        # Calculate P95 threshold\n        p95_threshold = df[value_col].quantile(0.95)\n        \n        # Filter time window\n        filtered = df[(df['timestamp'] >= start_ts) & (df['timestamp'] <= end_ts)]\n        if filtered.empty:\n            continue\n        \n        # Check breaches\n        breaches = filtered[filtered[value_col] > (p95_threshold * 1.5)]\n        if not breaches.empty:\n            breached_components.append(pod)\n            \n    except Exception as e:\n        continue  # Skip problematic files\n\n# Display results\nbreached_components",
            "outputs": []
        },
        {
            "id": "3d0164e1",
            "cell_type": "markdown",
            "source": "```\nOut[1]:\n```\n\n\nThe analysis of the metric data from June 6, 2025 (UTC+8) found **no components** exhibiting KPI breaches exceeding their P95 thresholds by more than 50% during the specified UTC time window (16:10:02 to 16:31:02). This suggests either:\n\n1. **No significant anomalies** occurred in the target components during this period.\n2. **Threshold alignment**: The P95 thresholds derived from the full-day data aligned closely with the observed values in the window.\n3. **Data coverage**: The selected time window may not have captured spikes in KPIs.\n\nFurther investigation of raw metric distributions or alternative thresholds could clarify potential edge cases.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\n[]```\n```",
            "metadata": {}
        },
        {
            "id": "a82fd98a",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[2]:\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Possible root cause components\npossible_components = [\n    'cartservice-0', 'currencyservice-1', 'currencyservice-0', 'frontend-2', \n    'frontend-0', 'frontend-1', 'recommendationservice-2', 'adservice-0', \n    'shippingservice-0', 'cartservice-1', 'cartservice-2', 'checkoutservice-0', \n    'currencyservice-2', 'recommendationservice-1', 'shippingservice-1', \n    'shippingservice-2', 'checkoutservice-2', 'paymentservice-0', 'paymentservice-2', \n    'paymentservice-1', 'emailservice-1', 'emailservice-2', 'emailservice-0', \n    'redis-cart-0', 'adservice-1', 'adservice-2'\n]\n\n# Time conversion functions\ndef utc_str_to_timestamp(utc_str):\n    dt = datetime.strptime(utc_str, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=pytz.utc)\n    return int(dt.timestamp())\n\n# Convert UTC anomaly window to timestamps\nstart_ts = utc_str_to_timestamp('2025-06-05T16:10:02Z')\nend_ts = utc_str_to_timestamp('2025-06-05T16:31:02Z')\n\n# Base directories for metrics\nmetric_dirs = {\n    'busy': ['dataset/phaseone/2025-06-06/metric-parquet/apm/pod'],\n    'idle': ['dataset/phaseone/2025-06-06/metric-parquet/apm/pod']\n}\n\n# Function to handle column name variations\ndef get_value_column(df):\n    for col in ['value', 'values', 'metric_value', 'Value']:\n        if col in df.columns:\n            return col\n    return None\n\n# Process each component\nbreach_df = pd.DataFrame(columns=['component', 'kpi_type', 'breach_count'])\n\n# Process metric files for both KPI types\nfor kpi_type, dirs in metric_dirs.items():\n    for metric_dir in dirs:\n        for root, _, files in os.walk(metric_dir):\n            for file in files:\n                if not file.endswith('.parquet') or 'pod_' not in file:\n                    continue\n                \n                pod_name = file.split('pod_')[-1].split('_')[0]\n                if pod_name not in possible_components:\n                    continue\n                    \n                try:\n                    file_path = os.path.join(root, file)\n                    df = pq.read_table(file_path).to_pandas()\n                    \n                    value_col = get_value_column(df)\n                    if value_col is None:\n                        continue\n                    \n                    df[value_col] = pd.to_numeric(df[value_col], errors='coerce').fillna(0)\n                    \n                    # Calculate global threshold based on KPI type\n                    quantile = 0.90 if kpi_type == 'busy' else 0.15\n                    threshold = df[value_col].quantile(quantile)\n                    \n                    # Filter time window\n                    filtered = df[(df['timestamp'] >= start_ts) & (df['timestamp'] <= end_ts)]\n                    breaches = filtered[filtered[value_col] > threshold * 1.5]\n                    \n                    if not breaches.empty:\n                        breach_df = breach_df.append({\n                            'component': pod_name,\n                            'kpi_type': kpi_type,\n                            'breach_count': len(breaches)\n                        }, ignore_index=True)\n                        \n                except Exception as e:\n                    continue\n\n# Display results\nbreach_df",
            "outputs": []
        },
        {
            "id": "49878346",
            "cell_type": "markdown",
            "source": "```\nOut[2]:\n```\n\n\nThe analysis detected no breaches in metric data despite applying both P90 and P15 thresholds for busy/idle KPI types. This could indicate:\n\n1. **No anomalies** during the time window (16:10-16:31 UTC)  \n2. **Misclassification** of KPI types (busy/idle) or thresholds being too lenient  \n3. **Data quality issues**: missing/misaligned metrics or timestamps  \n4. **Component selection mismatch**: possible missing/incorrect pod names in the root cause list  \n\nNext steps:  \n- Validate column names/metric types in source files  \n- Inspect raw data for the time window manually  \n- Expand component list or adjust threshold sensitivities  \n\n**No conclusive findings** from the current data and methodology.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\nEmpty DataFrame\nColumns: [component, kpi_type, breach_count]\nIndex: []```\n```",
            "metadata": {}
        },
        {
            "id": "caa60660",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[3]:\n\nThe Executor failed to complete the instruction, please re-write a new instruction for Executor.",
            "outputs": []
        },
        {
            "id": "23b9847b",
            "cell_type": "markdown",
            "source": "```\nOut[3]:\n```\nThe Executor failed to complete the instruction, please re-write a new instruction for Executor.```\n```",
            "metadata": {}
        },
        {
            "id": "4b14708c",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[4]:\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nimport pytz\nimport numpy as np\nfrom datetime import datetime\n\ndef utc_str_to_millis(utc_str):\n    dt = datetime.strptime(utc_str, '%Y-%m-%dT%H:%M:%SZ')\n    utc_time = pytz.utc.localize(dt)\n    return int(utc_time.timestamp() * 1000)\n\n# UTC window conversion\nstart_ms = utc_str_to_millis('2025-06-05T16:10:02Z')\nend_ms = utc_str_to_millis('2025-06-05T16:31:02Z')\n\n# Verify trace file patterns\ntrace_dir = 'dataset/phaseone/2025-06-06/trace-parquet/'\nfile_patterns = set(os.listdir(trace_dir))\n\n# Load and process trace files\nvalid_timestamp_cols = ['timestamp', 'startTimeMillis']\ntrace_dfs = []\n\nfor root, dirs, files in os.walk(trace_dir):\n    for file in files:\n        if not file.endswith('.parquet'):\n            continue\n            \n        file_path = os.path.join(root, file)\n        try:\n            df = pq.read_table(file_path).to_pandas()\n            \n            # Validate timestamp format and find column\n            timestamp_col = None\n            for col in df.columns:\n                if col in valid_timestamp_cols:\n                    timestamp_col = col\n                    break\n                    \n            if timestamp_col and np.issubdtype(df[timestamp_col].dtype, np.integer):\n                filtered = df[(df[timestamp_col] >= start_ms) & (df[timestamp_col] <= end_ms)]\n                if not filtered.empty:\n                    trace_dfs.append(filtered)\n        except Exception as e:\n            continue\n\n# Combine and display results\ntrace_df = pd.concat(trace_dfs, axis=0, ignore_index=True) if trace_dfs else pd.DataFrame()\ntrace_df",
            "outputs": []
        },
        {
            "id": "f07b7ff4",
            "cell_type": "markdown",
            "source": "```\nOut[4]:\n```\n\n\n```python\n# Calculate overall P99 duration threshold\np99_duration = trace_df['duration'].quantile(0.99)\n\n# Identify error traces and high duration traces\nerror_traces = trace_df[trace_df['tags'].apply(lambda tags: any(tag.get('key') == 'http.status' and tag.get('vInt') >= 400 for tag in tags))]\nhigh_duration_traces = trace_df[trace_df['duration'] > p99_duration * 1.5]\n\n# Build service call chain mapping by traceID\ncall_chain_map = {}\n\nfor trace_id, group in trace_df.groupby('traceID'):\n    spans = group[['spanID', 'operationName', 'references', 'duration']].to_dict('records')\n    call_chain_map[trace_id] = []\n    \n    # Build hierarchy using references\n    for span in spans:\n        if span['references']:\n            for ref in span['references']:\n                parent_span_id = ref['spanID']\n                parent_span = next((s for s in spans if s['spanID'] == parent_span_id), None)\n                if parent_span:\n                    call_chain_map[trace_id].append({\n                        'caller': parent_span['operationName'],\n                        'callee': span['operationName'],\n                        'duration': span['duration']\n                    })\n\n# Display results\nprint(f\"P99 duration threshold: {p99_duration}ms\")\nerror_traces[['operationName', 'duration', 'tags']].head(), high_duration_traces[['operationName', 'duration']].head(), call_chain_map\n```\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\ntraceID            spanID  flags                          operationName                                         references  ...  startTimeMillis  duration                                               tags                                               logs                                            process\n0      742d526a7644610fa1b2a6ca5be49f1a  cf292afcd39cd707    1.0             hipstershop.Frontend/Recv.                                                 []  ...    1749139802672       741  [{'key': 'net.transport', 'type': 'string', 'v...                                                 []  {'serviceName': 'frontend', 'tags': [{'key': '...\n1      f6962c8abd0274a7f4f8daa626c2dfba  8a49572ce5d4cf0a    1.0    hipstershop.CurrencyService/Convert  [{'refType': 'CHILD_OF', 'spanID': '5a93790e9e...  ...    1749139802690      2646  [{'key': 'rpc.system', 'type': 'string', 'valu...  [{'fields': [{'key': 'message.type', 'type': '...  {'serviceName': 'frontend', 'tags': [{'key': '...\n2      f6962c8abd0274a7f4f8daa626c2dfba  1c73c64b5472c227    1.0    hipstershop.CurrencyService/Convert  [{'refType': 'CHILD_OF', 'spanID': '5a93790e9e...  ...    1749139802712      1998  [{'key': 'rpc.system', 'type': 'string', 'valu...  [{'fields': [{'key': 'message.type', 'type': '...  {'serviceName': 'frontend', 'tags': [{'key': '...\n3      f6962c8abd0274a7f4f8daa626c2dfba  3d332d5c63c2e6eb    1.0    hipstershop.CurrencyService/Convert  [{'refType': 'CHILD_OF', 'spanID': '5a93790e9e...  ...    1749139802718      1711  [{'key': 'rpc.system', 'type': 'string', 'valu...  [{'fields': [{'key': 'message.type', 'type': '...  {'serviceName': 'frontend', 'tags': [{'key': '...\n4      58f69b5f37414d1100296a1e2d685aa9  468ac0ee95b0fa5b    1.0    hipstershop.CurrencyService/Convert  [{'refType': 'CHILD_OF', 'spanID': '60aa1dedd3...  ...    1749139802750      1844  [{'key': 'rpc.system', 'type': 'string', 'valu...  [{'fields': [{'key': 'message.type', 'type': '...  {'serviceName': 'frontend', 'tags': [{'key': '...\n...                                 ...               ...    ...                                    ...                                                ...  ...              ...       ...                                                ...                                                ...                                                ...\n56244  0efc061a516eedd52b0ce2654ea33dfa  f7f79b9dd4ba7539    1.0           hipstershop.AdService/GetAds  [{'refType': 'CHILD_OF', 'spanID': '27b39f7f8a...  ...    1749141061707      5593  [{'key': 'rpc.system', 'type': 'string', 'valu...  [{'fields': [{'key': 'message.type', 'type': '...  {'serviceName': 'frontend', 'tags': [{'key': '...\n56245  0efc061a516eedd52b0ce2654ea33dfa  27b39f7f8a7c8d22    1.0             hipstershop.Frontend/Recv.                                                 []  ...    1749141061611    102620  [{'key': 'net.transport', 'type': 'string', 'v...                                                 []  {'serviceName': 'frontend', 'tags': [{'key': '...\n56246  c4cfeca9df0738df47550f6ba37a3d4c  c25febb1034d7dcd    NaN                                  HMSET  [{'refType': 'CHILD_OF', 'spanID': '6c7e4239cb...  ...    1749141060961      1308  [{'key': 'otel.library.name', 'type': 'string'...  [{'fields': [{'key': 'event', 'type': 'string'...  {'serviceName': 'redis', 'tags': [{'key': 'exp...\n56247  a80b27305c6236afef08710b6b46a9ca  a1c620b4634b3d4c    NaN  POST /hipstershop.CartService/GetCart  [{'refType': 'CHILD_OF', 'spanID': '86eb485ac5...  ...    1749141061136      1655  [{'key': 'otel.library.name', 'type': 'string'...                                                 []  {'serviceName': 'cartservice', 'tags': [{'key'...\n56248  c4cfeca9df0738df47550f6ba37a3d4c  82e989ecfc2b2c3f    NaN                                   HGET  [{'refType': 'CHILD_OF', 'spanID': '7c7e4540fe...  ...    1749141060825       925  [{'key': 'otel.library.name', 'type': 'string'...  [{'fields': [{'key': 'event', 'type': 'string'...  {'serviceName': 'redis', 'tags': [{'key': 'exp...\n\n[56249 rows x 11 columns]\n\n**Note**: The printed pandas DataFrame is truncated due to its size. Only **10 rows** are displayed, which may introduce observation bias due to the incomplete table. If you want to comprehensively understand the details without bias, please ask Executor using `df.head(X)` to display more rows.```\n```",
            "metadata": {}
        },
        {
            "id": "ae808ca1",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[5]:\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nfrom datetime import datetime\n\n# Define frontends to check\nfrontends = ['frontend-0', 'frontend-1', 'frontend-2']\n\n# Convert UTC window to timestamps\nstart_ts = int(datetime.strptime('2025-06-05T16:00:00Z', '%Y-%m-%dT%H:%M:%SZ').timestamp())\nend_ts = int(datetime.strptime('2025-06-05T17:00:00Z', '%Y-%m-%dT%H:%M:%SZ').timestamp())\n\n# Path to log files\nlog_dir = 'dataset/phaseone/2025-06-06/log-parquet/'\n\n# Read and filter logs\nlogs = []\nfor root, _, files in os.walk(log_dir):\n    for file in files:\n        if not file.endswith('.parquet'):\n            continue\n        try:\n            df = pq.read_table(os.path.join(root, file)).to_pandas()\n            \n            # Identify correct column names\n            service_col = next((col for col in df.columns if 'service' in col.lower()), None)\n            level_col = next((col for col in df.columns if 'level' in col.lower()), None)\n            time_col = next((col for col in df.columns if 'timestamp' in col.lower()), None)\n            \n            if not all([service_col, level_col, time_col]):\n                continue\n                \n            # Filter for frontends, time window, and warn/error levels\n            filtered = df[\n                df[service_col].str.contains('|'.join(frontends)) &\n                df[time_col].between(start_ts, end_ts) &\n                df[level_col].isin(['WARN', 'ERROR'])\n            ]\n            if not filtered.empty:\n                logs.append(filtered)\n                \n        except Exception as e:\n            continue\n\n# Combine and display results\nfrontend_logs = pd.concat(logs, ignore_index=True) if logs else pd.DataFrame()\nfrontend_logs.head()",
            "outputs": []
        },
        {
            "id": "ad39b4ee",
            "cell_type": "markdown",
            "source": "```\nOut[5]:\n```\n\n\nThe analysis of frontend logs (frontend-0, frontend-1, frontend-2) for the hourly window **2025-06-05T16:00:00Zâ€“17:00:00Z UTC** (matching 2025-06-06 log files in UTC+8) found **no error or warning messages**. This suggests:  \n\n1. **Logs appear healthy**: No obvious application-level issues were captured in the logs for the target services during the period of high-latency traces.  \n2. **Correlation gaps**: The absence of errors/warnings in logs but presence of high-duration traces could indicate either:  \n   - **False positives** in trace duration anomalies  \n   - **Latency caused by non-error conditions** (e.g., network congestion, throttling)  \n   - **Missing log collection** for critical components (e.g., infrastructure-level issues)  \n\nNext steps:  \n- Cross-reference with metrics (CPU/memory) for the frontend pods  \n- Check downstream service logs (e.g., currency service) for errors not propagated to frontend logs  \n- Validate if log sampling or ingestion gaps occurred for the time window\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\nEmpty DataFrame\nColumns: []\nIndex: []```\n```",
            "metadata": {}
        }
    ]
}