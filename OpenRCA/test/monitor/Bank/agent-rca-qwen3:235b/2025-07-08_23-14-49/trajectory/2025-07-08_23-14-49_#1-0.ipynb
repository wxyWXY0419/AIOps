{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {},
    "cells": [
        {
            "id": "b02aeee9",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[1]:\n\nimport pandas as pd\n\n# Read metric_container.csv and filter by allowed components\ndf_container = pd.read_csv('dataset/Bank/telemetry/2021_03_04/metric/metric_container.csv')\n\nallowed_components = [\n    'apache01', 'apache02',\n    'Tomcat01', 'Tomcat02', 'Tomcat03', 'Tomcat04',\n    'MG01', 'MG02',\n    'IG01', 'IG02',\n    'Mysql01', 'Mysql02',\n    'Redis01', 'Redis02'\n]\n\ndf_filtered = df_container[df_container['cmdb_id'].isin(allowed_components)].copy()\n\n# Calculate P5 and P95 thresholds for each component-KPI pair\nthresholds_df = df_filtered.groupby(['cmdb_id', 'kpi_name'])['value'].agg(\n    p5=lambda x: x.quantile(0.05),\n    p95=lambda x: x.quantile(0.95)\n).reset_index()\n\n# Display the result\nthresholds_df",
            "outputs": []
        },
        {
            "id": "c55b0858",
            "cell_type": "markdown",
            "source": "```\nOut[1]:\n```\n\n\nThe analysis calculated 95th percentile (P95) and 5th percentile (P5) thresholds for 1,643 component-KPI pairs across all specified possible root cause components (e.g., Tomcat, Mysql, Redis, etc.). These thresholds represent the expected operating range of system metrics like CPU load, memory usage, and disk I/O for each component. For example:  \n\n- **IG01** has a P95 heap memory usage of 48.76 MB (vs. P5 at 10.68 MB), indicating typical memory consumption patterns.  \n- **Apache02** shows zero swap memory usage (Si/So metrics), suggesting no active swapping occurred during the day.  \n\nThese thresholds can serve as baselines to detect abnormalities. For instance, if a component's real-time metric exceeds its P95 value, it may indicate resource contention or performance degradation. The full dataset covers critical infrastructure components, ensuring comprehensive coverage of potential root causes.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\ncmdb_id                                          kpi_name            p5           p95\n0         IG01          JVM-Memory_7778_JVM_Memory_HeapMemoryMax  2.040136e+09  2.040136e+09\n1         IG01        JVM-Memory_7778_JVM_Memory_HeapMemoryUsage  1.068285e+01  4.875715e+01\n2         IG01         JVM-Memory_7778_JVM_Memory_HeapMemoryUsed  2.180178e+08  9.901924e+08\n3         IG01       JVM-Memory_7778_JVM_Memory_NoHeapMemoryUsed  8.265500e+08  9.432373e+08\n4         IG01         JVM-Operating System_7778_JVM_JVM_CPULoad  5.480000e-02  3.350250e-01\n...        ...                                               ...           ...           ...\n1638  apache02                      OSLinux-OSLinux_SWAP_SWAP_Si  0.000000e+00  0.000000e+00\n1639  apache02                      OSLinux-OSLinux_SWAP_SWAP_So  0.000000e+00  0.000000e+00\n1640  apache02  OSLinux-OSLinux_SYSTEM_SYSTEM_Check-DefaultRoute  1.000000e+00  1.000000e+00\n1641  apache02      OSLinux-OSLinux_SYSTEM_SYSTEM_Check-Hostname  1.000000e+00  1.000000e+00\n1642  apache02                OSLinux-OSLinux_ZABBIX_Host_Uptime  5.989793e+06  6.067511e+06\n\n[1643 rows x 4 columns]\n\n**Note**: The printed pandas DataFrame is truncated due to its size. Only **10 rows** are displayed, which may introduce observation bias due to the incomplete table. If you want to comprehensively understand the details without bias, please ask Executor using `df.head(X)` to display more rows.```\n```",
            "metadata": {}
        },
        {
            "id": "c5f072a7",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[2]:\n\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\n\n# Convert failure window times to epoch seconds in UTC+8\ntz = pytz.timezone('Asia/Shanghai')\nstart_dt = tz.localize(datetime(2021, 3, 4, 18, 0, 0))\nend_dt = tz.localize(datetime(2021, 3, 4, 18, 30, 0))\nstart_epoch = int(start_dt.timestamp())\nend_epoch = int(end_dt.timestamp())\n\n# Read and filter metric_container data for the failure window and allowed components\nallowed_components = [\n    'apache01', 'apache02', 'Tomcat01', 'Tomcat02', 'Tomcat03', 'Tomcat04',\n    'MG01', 'MG02', 'IG01', 'IG02', 'Mysql01', 'Mysql02', 'Redis01', 'Redis02'\n]\nfiltered_df = pd.read_csv('dataset/Bank/telemetry/2021_03_04/metric/metric_container.csv')\nfiltered_df = filtered_df[filtered_df['cmdb_id'].isin(allowed_components)]\nfiltered_df = filtered_df[(filtered_df['timestamp'] >= start_epoch) & (filtered_df['timestamp'] <= end_epoch)]\n\n# Merge with pre-calculated thresholds and identify anomalies\nmerged_df = pd.merge(filtered_df, thresholds_df, on=['cmdb_id', 'kpi_name'], how='left')\n\ndef calculate_breach(row):\n    value = row['value']\n    p5 = row['p5']\n    p95 = row['p95']\n    \n    if value < p5:\n        if p5 == 0:\n            return float('nan')  # Skip if p5=0 (non-negative KPIs)\n        return (p5 - value) / p5 * 100\n    elif value > p95:\n        if p95 == 0:\n            return 100.0  # Mark zero-based KPIs exceeding baseline as 100% breach\n        return (value - p95) / p95 * 100\n    else:\n        return float('nan')\n\nmerged_df['breach_percentage'] = merged_df.apply(calculate_breach, axis=1)\nanomalies_df = merged_df[merged_df['breach_percentage'].notnull()][['cmdb_id', 'kpi_name', 'timestamp', 'value', 'breach_percentage']]\n\n# Display the anomalies\nanomalies_df.sort_values('breach_percentage', ascending=False).head(20)",
            "outputs": []
        },
        {
            "id": "ff9fea98",
            "cell_type": "markdown",
            "source": "```\nOut[2]:\n```\n\n\nThe analysis detected anomalies in system metrics during the 30-minute failure window (18:00â€“18:30 UTC+8) across multiple components. Key findings include:\n\n1. **Mysql02 Disk I/O Spike**  \n   - **34,800% breach** in disk read throughput (`LOCALDISK-sdb_DSKRead`) far exceeding P95 thresholds, suggesting potential disk saturation or query overload.\n\n2. **Mysql01 Lock Contention**  \n   - **17,733% breach** in InnoDB row lock time (`Innodb Row Lock Time`), indicating severe resource contention or inefficient transactions.\n\n3. **Persistent Disk I/O and CPU Wait Issues**  \n   - Multiple components (Mysql, Redis, IG, Apache, Tomcat) showed **breach percentages exceeding 1000%** for disk-related KPIs (e.g., `DSKRead`, `DSKRTps`) and CPU wait I/O (`CPUWio`), signaling systemic resource contention.\n\n4. **Root Cause Hypotheses**  \n   - Mysql's disk I/O saturation likely cascaded into lock contention (Mysql01) and triggered CPU bottlenecks in dependent services (IG, Redis, Tomcat).  \n   - High CPU wait I/O across components suggests a possible shared storage bottleneck or network-attached storage latency.\n\n**Immediate action**: Investigate Mysql02's disk subsystem (sdb/sdc) performance and query patterns during this window. Validate if thresholds were calculated correctly (e.g., avoid zero-value baselines) and correlate with trace/log data for latency spikes.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\ncmdb_id                                         kpi_name   timestamp       value  breach_percentage\n699     Mysql02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdb_DSKRead  1614852000   46.533333       34800.087000\n778     Mysql01            Mysql-MySQL_3306_Innodb Row Lock Time  1614852060   35.666700       17733.350000\n7008       IG02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sda_DSKRead  1614852480  329.200000        5511.370013\n704     Mysql02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdc_DSKRead  1614852000    7.333333        5400.013500\n703     Mysql02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdc_DSKRTps  1614852000    0.916667        5389.023952\n9922    Redis02                           OSLinux-CPU_CPU_CPUWio  1614852660    3.565775        5079.048656\n17775      IG02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sda_DSKRTps  1614853200    6.050000        4437.511344\n691     Mysql02   OSLinux-OSLinux_LOCALDISK_LOCALDISK-sda_DSKBps  1614852000  179.000000        4375.000000\n13430      MG01  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sda_DSKRead  1614852900  266.800000        4346.666667\n9244    Redis02                           OSLinux-CPU_CPU_CPUWio  1614852600    2.417200        3410.820625\n157    Tomcat04                           OSLinux-CPU_CPU_CPUWio  1614852000    3.758100        2021.781843\n1215   Tomcat03                           OSLinux-CPU_CPU_CPUWio  1614852060    4.969000        1996.138027\n1042   Tomcat02                           OSLinux-CPU_CPU_CPUWio  1614852060    3.147700        1549.348948\n734    Tomcat01                           OSLinux-CPU_CPU_CPUWio  1614852060    3.441500        1479.394218\n7009       IG02  OSLinux-OSLinux_LOCALDISK_LOCALDISK-sda_DSKRTps  1614852480    2.050000        1437.503844\n13213   Redis02                           OSLinux-CPU_CPU_CPUWio  1614852900    1.058200        1436.964415\n1180   apache02                           OSLinux-CPU_CPU_CPUWio  1614852060    1.711817        1347.686583\n922     Redis01                           OSLinux-CPU_CPU_CPUWio  1614852060    0.819600        1263.702162\n362        IG01                           OSLinux-CPU_CPU_CPUWio  1614852000    2.386400        1241.276978\n1201       IG02                           OSLinux-CPU_CPU_CPUWio  1614852060    3.204500        1033.955756```\n```",
            "metadata": {}
        },
        {
            "id": "9aaaa230",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[3]:\n\nimport pandas as pd\n\n# Read and filter trace data for the failure window\ntrace_df = pd.read_csv('dataset/Bank/telemetry/2021_03_04/trace/trace_span.csv')\nfiltered_trace = trace_df[\n    (trace_df['timestamp'] >= 1614852000) & (trace_df['timestamp'] <= 1614853800)\n]\n\n# Extract high-latency spans (duration >1500 ms)\nhigh_lat_spans = filtered_trace[filtered_trace['duration'] > 1500]\n\n# Map span_id to rows for quick lookup\nspan_to_row = {row['span_id']: row for _, row in filtered_trace.iterrows()}\n\n# Correlate with Mysql01/02 in ancestry chains of high-latency spans\nfaulty_services = []\nfor _, span_row in high_lat_spans.iterrows():\n    current_span = span_row['span_id']\n    parent_id = span_row['parent_id']\n    chain = [current_span]\n    \n    # Build ancestry chain (span to root within filtered trace)\n    while parent_id in span_to_row:\n        chain.append(parent_id)\n        parent_id = span_to_row[parent_id]['parent_id']\n\n    # Find the deepest Mysql01/02 component in chain\n    for span in chain:\n        cmdb_id = span_to_row[span]['cmdb_id']\n        if cmdb_id in ['Mysql01', 'Mysql02']:\n            faulty_services.append({\n                'trace_id': span_row['trace_id'],\n                'faulty_component': cmdb_id\n            })\n            break  # Break after first match (deepest in chain)\n\n# Create DataFrame with explicit column definitions to avoid KeyError\ntrace_faulty_services = pd.DataFrame(faulty_services, columns=['trace_id', 'faulty_component'])\ntrace_faulty_services.drop_duplicates(inplace=True)\ntrace_faulty_services.sort_values('trace_id', inplace=True)\n\n# Display results\ntrace_faulty_services.head(20)",
            "outputs": []
        },
        {
            "id": "18e89133",
            "cell_type": "markdown",
            "source": "```\nOut[3]:\n```\n\n\nThe trace analysis found **no evidence** of Mysql01 or Mysql02 being part of high-latency call chains during the failure window. This suggests:  \n\n1. **Metrics vs. Traces Disconnect**:  \n   - Earlier metric anomalies (Mysql disk I/O, locks) did not directly correlate with trace-level latency in this dataset. Possible reasons:  \n     - Metrics captured database strain, but traces may have missed long-running queries or cross-component transactions.  \n     - Traces might not instrument database calls (e.g., only HTTP/API calls are traced, not database interactions).  \n\n2. **Alternative Root Causes**:  \n   - High-latency traces (not shown here) might originate from other components with anomalies (e.g., Redis CPU wait I/O at 5,079% breach or IG disk reads at 5,511% breach).  \n\n**Recommendations**:  \n- Cross-check with **log data** (e.g., slow query logs for MySQL, JVM thread dumps for Tomcat).  \n- Validate if database calls are **properly instrumented in traces** (e.g., check if span `cmdb_id` includes database connections).  \n- Refine trace analysis to **include all anomaly-causing components** (e.g., Redis, IG) and not just Mysql01/02.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\nEmpty DataFrame\nColumns: [trace_id, faulty_component]\nIndex: []```\n```",
            "metadata": {}
        },
        {
            "id": "c666f430",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "# In[4]:\n\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\n\n# Read and filter log_service.csv for target components and time window\ntarget_components = ['Mysql01', 'Mysql02', 'Redis02', 'IG02']\nlog_df = pd.read_csv('dataset/Bank/telemetry/2021_03_04/log/log_service.csv')\nfiltered_log = log_df[\n    (log_df['cmdb_id'].isin(target_components)) &\n    (log_df['timestamp'] >= 1614852000) & \n    (log_df['timestamp'] <= 1614853800)\n]\n\n# Define regex pattern for keywords\nkeywords = ['disk', 'I/O', 'lock', 'wait', 'timeout', 'swap']\npattern = '|'.join([f'(?i){kw}' for kw in keywords])  # Case-insensitive matches\n\n# Filter rows with keyword matches in 'value' column\nactionable_logs = filtered_log[filtered_log['value'].str.contains(pattern, na=False)]\n\n# Convert epoch to UTC+8 timestamp\ndef epoch_to_utc8(epoch):\n    return datetime.utcfromtimestamp(epoch).replace(tzinfo=pytz.utc).astimezone(pytz.timezone('Asia/Shanghai'))\n\nactionable_logs['human_time'] = actionable_logs['timestamp'].apply(epoch_to_utc8)\n\n# Return structured output with cmdb_id, timestamp, and value\nresult = actionable_logs[['cmdb_id', 'human_time', 'value']].sort_values(['cmdb_id', 'human_time'])\nresult.head(20)",
            "outputs": []
        },
        {
            "id": "fa5b044c",
            "cell_type": "markdown",
            "source": "```\nOut[4]:\n```\n\n\nThe log analysis found **no actionable entries** matching keywords like disk, I/O, lock, or timeout in **MySQL, Redis02, or IG02 logs** during the failure window. This lack of evidence suggests:\n\n1. **Potential Blind Spots**:  \n   - Critical events (e.g., MySQL disk I/O saturation, lock contention) may **not be logged** at the app/service log level or use different terminology (e.g., \"disk latency\" instead of \"I/O\").  \n   - Logs might require deeper parsing of structured fields (e.g., query execution time in MySQL slow logs) not included in this analysis.  \n\n2. **Next Steps**:  \n   - Cross-check with **MySQL-specific diagnostics**:  \n     - Analyze metric_container.csv for disk queue depths (e.g., `OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdX_DSKRun`) to confirm disk saturation.  \n     - Correlate high-latency traces (even if not linked to MySQL) with application logs (e.g., Tomcat/IG) for retry patterns or timeouts.  \n   - Validate log data completeness: Ensure logs from MySQL and Redis are being collected (e.g., check log file locations, rotation policies).  \n\nThe **metric data** ([previous step](#step-5)) strongly indicates disk I/O and lock issues in MySQL, even lacking direct log evidence. Prioritize metric-driven root cause analysis until logs are validated.\n\nThe original code execution output of IPython Kernel is also provided below for reference:\n\nEmpty DataFrame\nColumns: [cmdb_id, human_time, value]\nIndex: []```\n```",
            "metadata": {}
        }
    ]
}